{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "#np.random.seed(600)\n",
    "#np.random.seed(700)\n",
    "random_seed = 600\n",
    "np.random.seed(random_seed)\n",
    "import screed # Librería para leer archivos en FASTA/FASTQ\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "random.seed(random_seed)\n",
    "\n",
    "from numpy import linalg # SVD\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "#---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracy_model(history, skip):\n",
    "    # Plot training & validation accuracy values\n",
    "    #plt.plot(history.history['accuracy'])\n",
    "    #plt.plot(history.history['val_accuracy'])\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    plt.plot(np.arange(skip, len(accuracy), 1), accuracy[skip:])\n",
    "    plt.plot(np.arange(skip, len(val_accuracy), 1), val_accuracy[skip:])\n",
    "    plt.title('Model Train: Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss_model(history, skip):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    #plt.plot(history.history['loss'])\n",
    "    #plt.plot(history.history['val_loss'])\n",
    "    plt.plot(np.arange(skip, len(loss), 1), loss[skip:])\n",
    "    plt.plot(np.arange(skip, len(val_loss), 1), val_loss[skip:])\n",
    "    plt.title('Model Train: Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_class(df):\n",
    "    \n",
    "    class_ACP = []\n",
    "    class_noACP = []\n",
    "    \n",
    "    rows_model = df.shape[0]\n",
    "    \n",
    "    for x in range(int(rows_model/2)):\n",
    "        class_ACP.append(1)\n",
    "    \n",
    "    for x in range(int(rows_model/2)):\n",
    "        class_noACP.append(0)\n",
    "        \n",
    "    df['Class'] = class_ACP + class_noACP\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_index(df):\n",
    "    #Cambiar indices de filas y columnas por valores numéricos\n",
    "    total_rows_df = df.shape[0]\n",
    "    df.index = np.arange(0, total_rows_df)\n",
    "\n",
    "    total_columns_df = df.shape[1]\n",
    "    df.columns = np.arange(0, total_columns_df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(self):\n",
    "    if not os.path.exists('./weights'):\n",
    "        os.mkdir('./weights')\n",
    "    else:\n",
    "        #autoencoder.encoded_imgs.save('./weights/encoder_weights.h5')\n",
    "        #autoencoder.predicted.save('./weights/decoder_weights.h5')\n",
    "        self.save('./weights/ae_weights.h5') #Almacenar pesos del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracy(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    #plt.plot(hist_Deep_AE.history['val_accuracy'])\n",
    "    plt.title('Model Train: Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error(history, skip):\n",
    "    loss = history.history['loss']\n",
    "    #val_loss = history.history['val_loss']\n",
    "    plt.plot(np.arange(skip, len(loss), 1), loss[skip:])\n",
    "    #plt.plot(np.arange(skip, len(loss), 1), val_loss[skip:])\n",
    "    plt.title('Model Train: Loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_autoencoder(one_hot_sequences_ACP, one_hot_sequences_noACP):\n",
    "    \n",
    "    df_onehot_ACP = pd.DataFrame(one_hot_sequences_ACP) #Crear Dataframe de OneHot ACP\n",
    "    df_onehot_noACP = pd.DataFrame(one_hot_sequences_noACP) #Crear Dataframe de OneHot noACP\n",
    "    \n",
    "    dataframes = [df_onehot_ACP, df_onehot_noACP]\n",
    "    df = pd.concat(dataframes)\n",
    "    \n",
    "    filas_total_df = df.shape[0]\n",
    "    df.index = np.arange(0, filas_total_df) #Corregir los índices del nuevo dataframe\n",
    "    \n",
    "    dataset = df.values #Almacenar solo los valores en un nuevo dataframe\n",
    "    \n",
    "    in_out_dim = dataset.shape[1]  #Obtener el largo de los vectores leidos como input del modelo\n",
    "    \n",
    "    #Este es el tamaño de las representaciones encodeadas en el modelo\n",
    "    encoding_dim = 32 # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "    #Placegoholder de entrada\n",
    "    input_seq = Input(shape=(in_out_dim,))\n",
    "    #\"encoded\" es la representación encodeada de la entrada\n",
    "    encoded = Dense(128, activation='relu')(input_seq)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    #\"decoded\" es la reconstrucción con pérdidas de la entrada (específicamente de la cada encodeada)\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(in_out_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    #encoded = Dense(256, activation='relu')(input_seq)\n",
    "    #encoded = Dense(128, activation='relu')(encoded)\n",
    "    #encoded = Dense(64, activation='relu')(encoded)\n",
    "\n",
    "    #decoded = Dense(128, activation='relu')(encoded)\n",
    "    #decoded = Dense(256, activation='relu')(decoded)\n",
    "    #decoded = Dense(encoding_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    #Este modelo mapea una entrada a su reconstrucción\n",
    "    autoencoder = Model(input_seq, decoded)\n",
    "    #Este modelo mapea una entrada a una representación encodeada\n",
    "    encoder = Model(input_seq, encoded)\n",
    "    \n",
    "    # Crear un placeholder para una entrada de 32 dimensiones\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    #Recuperar la capa encodeada del modelo (layers[-3]) hasta llegar a la última de la parte de decodificación\n",
    "    decoder_layer = autoencoder.layers[-3](encoded_input)\n",
    "    decoder_layer = autoencoder.layers[-2](decoder_layer)\n",
    "    decoder_layer = autoencoder.layers[-1](decoder_layer)\n",
    "    #Crear el modelo de decodificación\n",
    "    decoder = Model(encoded_input, decoder_layer)\n",
    "    \n",
    "    return df, dataset, autoencoder, encoder, decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_descriptors(filepath_ACP_AAC,filepath_nonACP_AAC, filepath_ACP_DPC, filepath_noACP_DPC, filepath_ACP_PCP, filepath_noACP_PCP):\n",
    "    \n",
    "    #Leer Ficheros\n",
    "    #AAC\n",
    "    AAC_ACP = pd.read_csv(filepath_ACP_AAC, sep=\"\\t\", header=0)\n",
    "    AAC_noACP = pd.read_csv(filepath_nonACP_AAC, sep=\"\\t\", header=0)\n",
    "    #DPC\n",
    "    DPC_ACP = pd.read_csv(filepath_ACP_DPC, sep=\"\\t\", header=0)\n",
    "    DPC_noACP = pd.read_csv(filepath_noACP_DPC, sep=\"\\t\", header=0)\n",
    "    #PCP\n",
    "    PCP_ACP = pd.read_csv(filepath_ACP_PCP, sep=\"\\t\", header=0, error_bad_lines=False)\n",
    "    PCP_noACP = pd.read_csv(filepath_noACP_PCP, sep=\"\\t\", header=0, error_bad_lines=False)\n",
    "    \n",
    "    #Eliminar primera fila (nombre de secuencias. ej: seq_1...)\n",
    "    #ACC\n",
    "    AAC_ACP = AAC_ACP.drop(\"#\", axis=1)\n",
    "    AAC_noACP = AAC_noACP.drop(\"#\", axis=1)\n",
    "    #DPC\n",
    "    DPC_ACP = DPC_ACP.drop(\"#\", axis=1)\n",
    "    DPC_noACP = DPC_noACP.drop(\"#\", axis=1)\n",
    "    #PCP\n",
    "    PCP_ACP = PCP_ACP.drop(\"FASTA_NAME\", axis=1)\n",
    "    PCP_noACP = PCP_noACP.drop(\"FASTA_NAME\", axis=1)\n",
    "\n",
    "    #Unir Conjuntos de características por la derecha: AAC - DPC -PCP\n",
    "    df_ACP_feature = AAC_ACP.join(DPC_ACP, how='right', lsuffix='', rsuffix='_onehot', sort=False)\n",
    "    df_noACP_feature = AAC_noACP.join(DPC_noACP, how='right', lsuffix='', rsuffix='_onehot', sort=False)\n",
    "\n",
    "    df_ACP_feature = df_ACP_feature.join(PCP_ACP, how='right', lsuffix='', rsuffix='_onehot', sort=False)\n",
    "    df_noACP_feature = df_noACP_feature.join(PCP_noACP, how='right', lsuffix='', rsuffix='_onehot', sort=False)\n",
    "\n",
    "    #Concatenar dataframes de clase positiva con negativa\n",
    "    #dataframes_features = [df_ACP_feature, df_noACP_feature]\n",
    "    #df_features = pd.concat(dataframes_features)\n",
    "\n",
    "    #(1516, 660)\n",
    "    #print(df_features.shape)\n",
    "    \n",
    "    #Cambiar indices de filas y columnas por valores numéricos\n",
    "    #total_rows_df = df_features.shape[0]\n",
    "    #df_features.index = np.arange(0, total_rows_df)\n",
    "\n",
    "    #total_columns_df = df_features.shape[1]\n",
    "    #df_features.columns = np.arange(0, total_columns_df)\n",
    "    \n",
    "    #return df_features\n",
    "    \n",
    "    return df_ACP_feature, df_noACP_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(sequences,alphabet,padding):\n",
    "    \n",
    "    one_hot_sequence_list = []\n",
    "    alphabet_one_hot = {}\n",
    "    \n",
    "    alphabet_lenght = len(alphabet)\n",
    "    \n",
    "    max_length = max(len(x) for x in sequences)\n",
    "        \n",
    "    for i in range(len(alphabet)): #Crear los array de dimension 20 llenados con cero para cada una de las letras del abecedario de AA\n",
    "        alphabet_one_hot[alphabet[i]] = [0]*alphabet_lenght #Agregar un arreglo (20-dimensional) para cada letra (posición i dentro del diccionario \"alphabet\")\n",
    "        alphabet_one_hot[alphabet[i]][i] = 1 #Agregar un 1 en la posición que corresponda a cada letra en el arreglo creado anteriormente\n",
    "\n",
    "    for i in range((len(sequences))): #Recorre cada una de las secuencias. 1516 en total (758 ACP - 758 noACP)\n",
    "        one_hot_sequence = [] #Arreglo de Arreglos, que almacena cada una de las letras pero en su versión One Hot\n",
    "        for j in range(len(sequences[i])): #Recorrer cada una de las letras (i) de una secuencia (secuences[i])\n",
    "            #Dependiendo de la letra leida, agrega el arreglo de 20 dimensiones creado anteriormente que corresponde a dicha letra en cuestión\n",
    "            #A = [1,0,...,0], C = [0,1,0,...,0], etc.\n",
    "            one_hot_sequence = one_hot_sequence + alphabet_one_hot[sequences[i][j]]\n",
    "        \n",
    "        if (max_length<=padding): # Hacer Padding unicamente si el valor es <= al valor de la secuencia más larga\n",
    "            for k in range(padding-len(sequences[i])):\n",
    "                one_hot_sequence = one_hot_sequence + [0]*alphabet_lenght #Llenar los espacios faltantes\n",
    "        one_hot_sequence_list.append(one_hot_sequence)\n",
    "\n",
    "    #return np.array(one_hot_sequence_list)\n",
    "    return one_hot_sequence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd(sequences, sparse_matrix, alphabet):\n",
    "    from numpy import linalg\n",
    "    \n",
    "    sparse_matrix_SVD = []\n",
    "    tri_feature = [0] * len(alphabet) # Largo del vector resultante pero también es la dimensión de U (filasxfilas de M original)\n",
    "    \n",
    "    for i in range(len(sparse_matrix)):\n",
    "        sequence_lenght = len(sequences[i])\n",
    "        U, s, Vt = linalg.svd(sparse_matrix[i])\n",
    "        for j in range(len(s)):\n",
    "            tri_feature = tri_feature + U[j] * s[j] / sequence_lenght # Multiplica cada una de las filas de U por los elementos de s y entrega la sumatoria\n",
    "        sparse_matrix_SVD.append(tri_feature)\n",
    "    return sparse_matrix_SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_matrix(alphabet_kmers,sequences_kmers):\n",
    "    \n",
    "    list_sparse_matrix = [] #Lista que contiene todas las sparse matrix generadas\n",
    "\n",
    "    for i in range(len(sequences_kmers)): \n",
    "\n",
    "        sparse_matrix = [] # Inicializacion de la sparse matrix de cada secuencia\n",
    "\n",
    "        for x in range(len(alphabet_kmers)):\n",
    "            sparse_matrix.append([]) # Cantidad de filas que contendrá la matriz (numero de letras del alfabeto ^ K)\n",
    "\n",
    "        for j in range(len(sequences_kmers[i])): # Avanza entre los k-mers de una misma secuencia\n",
    "            for k in range(len(alphabet_kmers)): # Compara cada k-mers con las distintas posibilidades de combinaciones del alfabeto\n",
    "                if (sequences_kmers[i][j] == alphabet_kmers[k]): # Comparacion de los K-mers de cada secuencia con las posibles combinaciones del alfabeto\n",
    "                    sparse_matrix[k].append(1) #1: si son iguales\n",
    "                else:\n",
    "                    sparse_matrix[k].append(0)#0: si no son iguales\n",
    "        list_sparse_matrix.append(sparse_matrix) # Agregación de cada sparse matrix generada para cada secuencia a la lista\n",
    "    return list_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_sequence(all_sequences, alphabet):\n",
    "    all_sequences_reduced = [] # Almacena todas las secuencias codificadas y la devuelve a la función principal\n",
    "    number_groups = len(alphabet.keys()) # Cantidad de Grupos que contiene el nuevo alfabeto\n",
    "    number_sequences = len(all_sequences) # Cantidad de Secuencias a Codificar\n",
    "    for i in range(number_sequences): # Recorrer todas las secuencias\n",
    "        count = 0 # Contador = 0: Se está examinando la primera letra de la secuencia; Contador !=0: Se extá examinando cualquier otra letra\n",
    "        first_letter_found = False # Verifica si se ha encontrado la primera letra dentro del diccionario\n",
    "        next_letter_found = False # Verifica si se ha encontrado la letra \"i\" dentro de la secuencia a codificar\n",
    "        index = 1 # Indice que recorre las \"keys\" del diccionario que contiene los grupos de letras (alphabet)\n",
    "        for j in range(len(all_sequences[i])): # Recorre cada una de las letras de la secuencia seleccionada\n",
    "            if (count==0):\n",
    "                while ((first_letter_found==False) and (index <= number_groups)): # Asignar la primera letra codificada del alfabeto a la nueva secuencia\n",
    "                    if (alphabet[index].count(all_sequences[i][j])): # Verifica si la letra se encuentra dentro del diccionario\n",
    "                        all_sequences_reduced.append(str(index))\n",
    "                        first_letter_found = True # Se encontró la primera letra de la secuencia a codificar\n",
    "                        count = count + 1\n",
    "                    else:\n",
    "                        index = index + 1 # No se encontró la primera letra, se busca en el siguiente grupo (index + 1)\n",
    "            else:\n",
    "                index = 1\n",
    "                while ((next_letter_found==False) and (index <= number_groups)):\n",
    "                    if (alphabet[index].count(all_sequences[i][j])): \n",
    "                        aux = all_sequences_reduced[i] + str(index)\n",
    "                        all_sequences_reduced[i] = aux\n",
    "                        next_letter_found = True\n",
    "                    else:\n",
    "                        index = index + 1\n",
    "            next_letter_found = False         \n",
    "    return all_sequences_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dictionary(dictionary):\n",
    "    #groups = alphabet_reduction[\"DipoleMoment\"].values()\n",
    "    #Dictionary = key1:[values1], key2:[values2],..., keyn:[valuesn]\n",
    "    sorted_list = []\n",
    "    key_list = [] #Lista que almacena las nuevas Key ordenadas del Diccionario ordenado\n",
    "    dic_result = {}\n",
    "    for lenght in range(len(dictionary)): #Cantidad de grupos en esa codificación = len(dictionary)\n",
    "        key_list.append(lenght+1) #Se agrega cada una de las key ordenadas a la lista\n",
    "    for dic in dictionary: #Extrae cada uno de los valores (dic) que corresponden a vectores en el diccionario\n",
    "        sorted_list.append(sorted(dic)) #Ordena los valores dentro de los vectores en orden de menor a mayor (letra o número)\n",
    "    sorted_list = sorted(sorted_list) #Una vez ordenados los valores dentro de los vectores, se ordenan ahora los vectores\n",
    "    index = 0\n",
    "    for values in sorted_list:\n",
    "        dic_result[int(key_list[index])] = values #Asignamos cada unos de los vectores (values) ordenados a cada una de las key ordenadas\n",
    "        index = index + 1\n",
    "    return dic_result #Se devuelve un nuevo diccionario ordenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kmers(sequence, ksize):\n",
    "    kmers = []\n",
    "    n_kmers = len(sequence) - ksize + 1 # Cantidad de K-mers totales por secuencia = L - k + 1\n",
    "    \n",
    "    for i in range(n_kmers):\n",
    "        kmer = sequence[i:i + ksize] #Selecciona la cantidad de letras especificadas por \"ksize\"\n",
    "        kmers.append(kmer) #Cada una de las posiciones almacena un k-mer de tamaño k\n",
    "\n",
    "    return kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kmers_from_sequences(sequences, ksize):\n",
    "    all_kmers = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        if (len(sequence) >= ksize): #El valor de K siempre debe ser menor al largo de las secuencias\n",
    "            kmers = build_kmers(sequence, ksize)\n",
    "            #all_kmers += kmers\n",
    "            all_kmers.append(kmers)   \n",
    "    return all_kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_sequences(filename):\n",
    "    all_sequences = []\n",
    "    for record in screed.open(filename):\n",
    "        sequence = record.sequence\n",
    "        all_sequences.append(sequence)   \n",
    "    return all_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet_operations(K, encoded_word):\n",
    "    \n",
    "    alphabet = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y'] #B-J-O-U-X-Z NO PRESENTES\n",
    "    \n",
    "    #Codificacion del alfabeto de aa en grupos definido por sus características fisico-químicas\n",
    "    alphabet_reduction = { \"Hydrophobicity\" : {1: ['R','K','E','D','Q','N'], 2:['P','H','Y','G','A','S','T'],\n",
    "                                           3:['M','F','W','C','V','L','I']},\n",
    "                         \"Polarizability\": {1: ['G','A','S','D','T'], 2:['E','Q','I','L','C','P','N','V'], \n",
    "                                            3:['R','Y','W','K','M','H','F']},\n",
    "                         \"VanWaals\": {1: ['G','A','S','C','T','P','D'], 2:['N','V','E','Q','I','L'],\n",
    "                                      3:['F','R','Y','W','M','H','K']},\n",
    "                         \"Polarity\": {1: ['C','M','V','Y','L','I','F','W'], 2:['P','A','T','G','S'],\n",
    "                                      3:['K','N','E','D','H','Q','R']},\n",
    "                         \"DipoleMoment\": {1: ['A','G','V'], 2: ['I','L','F','P'], 3: ['Y','M','T','S'], \n",
    "                                           4: ['H','N','Q','W'], 5: ['R','K'], 6:['D','E'], 7: ['C']}           \n",
    "    }\n",
    "    \n",
    "    #Creación de K-mers presentes en el alfabeto de AA\n",
    "    alphabet_kmers = [''.join(i) for i in itertools.product(alphabet, repeat = K)]\n",
    "    alphabet_kmers = sorted(alphabet_kmers)\n",
    "    \n",
    "    #Codificacion del alfabeto\n",
    "    alphabet_reduced = reduced_sequence(alphabet, alphabet_reduction[encoded_word])\n",
    "    \n",
    "    # Eliminar \"letras\" repetidas\n",
    "    alphabet_reduced = list(dict.fromkeys(alphabet_reduced))\n",
    "    alphabet_kmers_reduced = [''.join(i) for i in itertools.product(alphabet_reduced, repeat = K)]\n",
    "    alphabet_kmers_reduced = sorted(alphabet_kmers_reduced )\n",
    "    \n",
    "    return alphabet, alphabet_reduction, alphabet_kmers, alphabet_reduced, alphabet_kmers_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACP():\n",
    "    \n",
    "    #Parametros a Modificar\n",
    "    K = 3\n",
    "    # \"Hydrophobicity\" \"Polarizability\" \"VanWaals\" \"Polarity\" \"DipoleMoment\"\n",
    "    encoded_word = \"Polarizability\"\n",
    "    #encoded_word = \"Hydrophobicity\"\n",
    "    #encoded_word = \"VanWaals\"\n",
    "    #encoded_word = \"Polarity\"\n",
    "    #encoded_word = \"DipoleMoment\"\n",
    "    \n",
    "    alphabet, alphabet_reduction, alphabet_kmers, alphabet_reduced, alphabet_kmers_reduced = alphabet_operations(K, encoded_word)\n",
    "    \n",
    "    # Largo Combinación de Alfabeto = (Cantidad de Letras en el Alfabeto)^(K)\n",
    "    #len(alphabet_kmers)\n",
    "    \n",
    "    #path_ACP = 'Databases/ACP.fasta'\n",
    "    #path_noACP = 'Databases/noACP.fasta'\n",
    "    path_ACP = '../Databases/ACP.fasta'\n",
    "    path_noACP = '../Databases/noACP.fasta'\n",
    "    \n",
    "    # Leyendo Secuencias desde archivo fasta y almacenando secuencias\n",
    "    all_sequences_ACP = read_all_sequences(path_ACP) # ACPs\n",
    "    all_sequences_noACP = read_all_sequences(path_noACP) # noACPs\n",
    "    #print('Cantidad de Secuencias Leidas: ACP ',len(all_sequences_ACP), '- noACP: ',len(all_sequences_noACP))\n",
    "    \n",
    "    alphabet_reduction[encoded_word] = sort_dictionary(alphabet_reduction[encoded_word].values())\n",
    "    \n",
    "    #Codificación de Secuencias\n",
    "    all_sequences_ACP_reduced = reduced_sequence(all_sequences_ACP, alphabet_reduction[encoded_word])\n",
    "    all_sequences_noACP_reduced = reduced_sequence(all_sequences_noACP, alphabet_reduction[encoded_word])\n",
    "    \n",
    "    #alphabet_reduction[encoded_word]\n",
    "    \n",
    "    #Creando k-mers reducidos\n",
    "    sequences_kmers_ACP_reduced = read_kmers_from_sequences(all_sequences_ACP_reduced, K) # ACPs\n",
    "    sequences_kmers_noACP_reduced = read_kmers_from_sequences(all_sequences_noACP_reduced, K) # noACPs\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------------------------\n",
    "    #K-MERS SPARSE MATRIX\n",
    "    \n",
    "    #Sparse Matrix co alfabeto reducido\n",
    "    list_sparse_matrix_ACP_reduced = create_sparse_matrix(alphabet_kmers_reduced, sequences_kmers_ACP_reduced)\n",
    "    list_sparse_matrix_noACP_reduced = create_sparse_matrix(alphabet_kmers_reduced, sequences_kmers_noACP_reduced)\n",
    "    \n",
    "    #Transformar las Sparse Matrix en vectores mediante el algoritmo SVD\n",
    "    sparse_matrix_SVD_ACP = svd(all_sequences_ACP_reduced, list_sparse_matrix_ACP_reduced, alphabet_kmers_reduced)\n",
    "    sparse_matrix_SVD_noACP = svd(all_sequences_noACP_reduced, list_sparse_matrix_noACP_reduced, alphabet_kmers_reduced)\n",
    "    \n",
    "    print('ACP- Cantidad de Sparse-Matrix: ', len(sparse_matrix_SVD_ACP), '- Largo Sparse Matrix: ', len(sparse_matrix_SVD_ACP[0]))\n",
    "    print('noACP- Cantidad de Sparse-Matrix: ', len(sparse_matrix_SVD_noACP), '- Largo Sparse Matrix: ', len(sparse_matrix_SVD_noACP[0]))\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------------------------\n",
    "    #One Hot Encoding\n",
    "    \n",
    "    #One Hot Encoding de secuencias originales\n",
    "    one_hot_sequences_ACP = one_hot_encoding(all_sequences_ACP, alphabet, 50)\n",
    "    one_hot_sequences_noACP = one_hot_encoding(all_sequences_noACP, alphabet, 50)\n",
    "    \n",
    "    #np.array(one_hot_sequences_ACP).shape\n",
    "    \n",
    "    ##df_model_ACP = sparse_matrix_SVD_ACP.join(one_hot_sequences_ACP, how='right', lsuffix='', rsuffix='_onehot', sort=False)\n",
    "    ##df_model_noACP = sparse_matrix_SVD_noACP.join(one_hot_sequences_noACP, how='right', lsuffix='', rsuffix='_onehot', sort=False)\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    #Feature Extraction: AAC - DPC\n",
    "    \n",
    "    #AAC\n",
    "    filepath_ACP_AAC = \"../Databases/AAC_ACP.tsv\" #ACP AAC\n",
    "    filepath_nonACP_AAC = \"../Databases/AAC_noACP.tsv\" #nonACP AAC\n",
    "    #DPC\n",
    "    filepath_ACP_DPC = \"../Databases/DPC_ACP.tsv\" #ACP AAC\n",
    "    filepath_noACP_DPC = \"../Databases/DPC_noACP.tsv\" #nonACP AAC\n",
    "    #PCP\n",
    "    filepath_ACP_PCP = \"../Databases/PCP_ACP_fixed.txt\" #ACP AAC\n",
    "    filepath_noACP_PCP = \"../Databases/PCP_noACP_fixed.txt\" #nonACP AAC\n",
    "    \n",
    "    df_ACP_feature, df_noACP_feature = read_descriptors(filepath_ACP_AAC,filepath_nonACP_AAC, filepath_ACP_DPC, \n",
    "                                                        filepath_noACP_DPC, filepath_ACP_PCP, filepath_noACP_PCP)\n",
    "    \n",
    "    ##df_model_ACP = df_model_ACP.join(df_ACP_feature, how='right', lsuffix='', rsuffix='_feature', sort=False)\n",
    "    ##df_model_noACP = df_model_noACP.join(df_noACP_feature, how='right', lsuffix='', rsuffix='_feature', sort=False)\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------\n",
    "    #Deep Autoencoder\n",
    "    \n",
    "    df, dataset, autoencoder, encoder, decoder = build_deep_autoencoder(one_hot_sequences_ACP, one_hot_sequences_noACP)\n",
    "    \n",
    "    #autoencoder.summary()\n",
    "    #encoder.summary()\n",
    "    #decoder.summary()\n",
    "    \n",
    "    #autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    epochs = 1000\n",
    "    batch_size = 256\n",
    "    \n",
    "    skip = 0\n",
    "    \n",
    "    hist_Deep_AE = autoencoder.fit(dataset, dataset,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    shuffle = False, verbose=0)\n",
    "    \n",
    "    #show_error(hist_Deep_AE, 0) #skip = numero de epochs que salta\n",
    "    #show_accuracy(hist_Deep_AE)\n",
    "    \n",
    "    #LAYERS INFO\n",
    "    #for layer in autoencoder.layers: \n",
    "      #print(layer.get_config())\n",
    "        \n",
    "    encoded_seq = encoder.predict(dataset)\n",
    "    decoded_seq = decoder.predict(encoded_seq)\n",
    "    predicted = autoencoder.predict(dataset)\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------\n",
    "    #Create Input Dataframe for the CNN model\n",
    "    \n",
    "    #len(sparse_matrix_SVD_ACP[0]) + len(one_hot_sequences_ACP[0]) + df_ACP_feature.shape[1] + len(encoded_seq[0])\n",
    "    \n",
    "    df_sparse_ACP = DataFrame(sparse_matrix_SVD_ACP)\n",
    "    df_sparse_noACP = DataFrame(sparse_matrix_SVD_noACP)\n",
    "    \n",
    "    df_onehot_ACP = DataFrame(one_hot_sequences_ACP)\n",
    "    df_onehot_noACP = DataFrame(one_hot_sequences_noACP)\n",
    "    \n",
    "    df_model_ACP = df_sparse_ACP.join(df_onehot_ACP, how='right', lsuffix='', rsuffix='_onehot', sort=False)\n",
    "    df_model_noACP = df_sparse_noACP.join(df_onehot_noACP, how='right', lsuffix='', rsuffix='_onehot', sort=False)\n",
    "    \n",
    "    #df_model_ACP = df_model_ACP.join(df_ACP_feature, how='right', lsuffix='', rsuffix='_feature', sort=False)\n",
    "    #df_model_noACP = df_model_noACP.join(df_noACP_feature, how='right', lsuffix='', rsuffix='_feature', sort=False)\n",
    "    \n",
    "    df_model = pd.concat([df_model_ACP, df_model_noACP])\n",
    "    \n",
    "    df_model = change_index(df_model)\n",
    "    \n",
    "    df_encoded = DataFrame(encoded_seq)\n",
    "    \n",
    "    df_model = df_model.join(df_encoded, how='right', lsuffix='', rsuffix='_ae', sort=False)\n",
    "    \n",
    "    df_model = change_index(df_model)\n",
    "    \n",
    "    df_model = add_class(df_model)\n",
    "    \n",
    "    return (df_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACP- Cantidad de Sparse-Matrix:  758 - Largo Sparse Matrix:  27\n",
      "noACP- Cantidad de Sparse-Matrix:  758 - Largo Sparse Matrix:  27\n"
     ]
    }
   ],
   "source": [
    "df_model = ACP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1516, 1060)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.to_csv('../Databases/Datasets with differents encoded types/No Feature/nf_polarizability.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
